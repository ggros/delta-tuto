{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01fe185e-a7b7-45a7-a1ed-e95e420cacfb",
   "metadata": {},
   "source": [
    "# Turoriel\n",
    "\n",
    "https://www.margo-group.com/fr/actualite/tutoriel-delta-lake-premiere-prise-en-main/\n",
    "\n",
    "## Init Spark\n",
    "L’initialisation de la session Spark s’écrit comme cela avec le noyau Spylon que j’utilise pour ce notebook.\n",
    "\n",
    "Il faut veiller à inclure le jar Delta Lake et à définir des configurations additionnelles.\n",
    "\n",
    "« io.delta.sql.DeltaSparkSessionExtension » permet l’utilisation des fonctionnalités de Delta Lake à travers la syntaxe spark.sql et\n",
    "\n",
    "« org.apache.spark.sql.delta.catalog.DeltaCatalog » permet l’interaction avec un metastore de base de données et de tables.\n",
    "\n",
    "Note: pour spark 3.1.2 il faut utiliser delta-core 1.0.0 et pas 0.7.0 comme dans l'article original spark 3.0\n",
    "\n",
    "`bin/spark-shell --packages io.delta:delta-core_2.12:1.0.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\"`\n",
    "\n",
    "https://docs.delta.io/latest/quick-start.html#spark-scala-shell\n",
    "\n",
    "https://jupyter-docker-stacks.readthedocs.io/en/latest/using/specifics.html#local-mode-in-scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a93be6-ef5a-4e9d-b2ed-e8791bc2847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "#Configure Spark to use a local master\n",
    "launcher.master = \"local\"\n",
    "\n",
    "#launcher.num_executors = 1\n",
    "#launcher.executor_cores = 1\n",
    "launcher.driver_memory = '4g'\n",
    "launcher.conf.set(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\")\n",
    "launcher.conf.set(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "launcher.conf.set(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cefbe24-3426-4a0c-8116-0f87778a9998",
   "metadata": {},
   "source": [
    "Ci-dessous, nous notons la ligne import io.delta.sql.DeltaSparkSessionExtension qui nous permettra d’interagir programmatiquement avec les tables au format Delta :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48d4abf7-15f7-4cf2-8109-404d0d89e91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://cd6256a52912:4040\n",
       "SparkContext available as 'sc' (version = 3.0.2, master = local, app id = local-1634066963096)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: String = version 2.12.10\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// check scala version\n",
    "util.Properties.versionString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ad151b-9ec6-4a61-8d20-a34856516a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:26\n",
       "res1: Double = 5050.0\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Sum of the first 100 whole numbers\n",
    "val rdd = sc.parallelize(0 to 100)\n",
    "rdd.sum()\n",
    "// 5050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54a6e5d-016a-4c58-b683-b67c28b87d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.io.File\n",
       "import scala.reflect.io.Directory\n",
       "import org.apache.spark.sql.Encoders\n",
       "import io.delta.sql.DeltaSparkSessionExtension\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import java.io.File\n",
    "import scala.reflect.io.Directory\n",
    "import org.apache.spark.sql.Encoders\n",
    "\n",
    "import io.delta.sql.DeltaSparkSessionExtension"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da4f8ec-51ff-499a-b4d9-ef49b0644bf5",
   "metadata": {},
   "source": [
    "Nous définissons des variables pour la gestion de nos dossiers de travail. Le dossier raw contient les données brutes, le dossier parquet contient les fichiers sous-jacents à la table au format Parquet que nous allons créer et le dossier delta contient les fichiers sous-jacents à la table au format Delta que nous allons créer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a5c70d-cbbf-45b7-9b3f-03c265b2c315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataFolder: String = /home/jovyan/data/\n",
       "rawDataFolder: String = /home/jovyan/data/raw/\n",
       "rawDataFile: String = /home/jovyan/data/raw/fake_people.csv\n",
       "parquetDataFolder: String = /home/jovyan/data/parquet/\n",
       "deltaDataFolder: String = /home/jovyan/data/delta/\n",
       "deltaLogFolder: String = /home/jovyan/data/delta/_delta_log/\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataFolder: String = \"/home/jovyan/data/\"\n",
    "val rawDataFolder: String = s\"${dataFolder}raw/\"\n",
    "val rawDataFile: String = s\"${rawDataFolder}fake_people.csv\"\n",
    "val parquetDataFolder: String = s\"${dataFolder}parquet/\"\n",
    "val deltaDataFolder: String = s\"${dataFolder}delta/\"\n",
    "val deltaLogFolder: String = s\"${deltaDataFolder}_delta_log/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb862292-8070-4d4d-80ba-27ffb5ef180c",
   "metadata": {},
   "source": [
    "Nous vidons les dossiers pour retrouver le même état initial à chaque run :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08e8775a-3830-4747-a637-c10ef24d1a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "directoryRaw: scala.reflect.io.Directory = /home/jovyan/data/raw\n",
       "directoryParquet: scala.reflect.io.Directory = /home/jovyan/data/parquet\n",
       "directoryDelta: scala.reflect.io.Directory = /home/jovyan/data/delta\n",
       "directoryDeltaLog: scala.reflect.io.Directory = /home/jovyan/data/delta/_delta_log\n",
       "res16: scala.reflect.io.Directory = /home/jovyan/data/delta\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val directoryRaw = new Directory(new File(rawDataFolder))\n",
    "val directoryParquet = new Directory(new File(parquetDataFolder))\n",
    "val directoryDelta = new Directory(new File(deltaDataFolder))\n",
    "val directoryDeltaLog = new Directory(new File(deltaLogFolder))\n",
    "directoryParquet.deleteRecursively()\n",
    "directoryDelta.deleteRecursively()\n",
    "directoryParquet.createDirectory()\n",
    "directoryDelta.createDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165de5eb-f371-48eb-924c-f7fac465cc45",
   "metadata": {},
   "source": [
    "Enfin, nous définissons une courte fonction pour afficher le contenu d’un dossier donné. Nous utiliserons un jeu de donnée fictif créé avec la bibliothèque Python Faker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9819fe3c-5846-4110-a959-145b51f3fbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/data/raw/fake_people.csv 1.178365 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "showFilesInDir: (dir: scala.reflect.io.Directory)Unit\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def showFilesInDir(dir: Directory): Unit = {\n",
    "  val it = for { file <- dir.files; if !(file.toString.contains(\"/.\")) }\n",
    "    yield f\"${file} ${file.length.toDouble / 1000000} MB\"\n",
    "  it foreach println\n",
    "}\n",
    "showFilesInDir(directoryRaw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293013ad-bb70-4bfd-b3cf-1b6a78e35f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17784dc8-9561-454e-8cff-5c21622abf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+--------------------+--------------------+-------------------+---------+\n",
      "|  id|                name|               email|             address|                city|           dateTime|randomInt|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-------------------+---------+\n",
      "|1000|       David Farrell|bairdmichelle@exa...|560 Edward Glens ...|        Colemanburgh|2005-05-20 00:00:00|     1494|\n",
      "|1001|        Daniel Scott|michellemccormick...|672 Denise Glen\n",
      "P...|           Port Jody|1996-05-23 00:00:00|     8577|\n",
      "|1002|    Heather Thompson|  wjames@example.com|8537 Juan Mountai...|South Stephanieshire|2019-01-17 00:00:00|     4966|\n",
      "|1003|Dr. Scott Morgan DDS|  uburch@example.com|66801 Cunningham ...|          Danielberg|1997-09-16 00:00:00|     5824|\n",
      "|1004|     Jennifer Torres|randall51@example...|PSC 0139, Box 045...|        Lake Jessica|2007-02-11 00:00:00|     6989|\n",
      "|1005|  Jennifer Rodriguez| zharris@example.org|948 Flores Knoll ...|      New Aprilhaven|1999-08-19 00:00:00|     7485|\n",
      "|1006|     Andrew Anderson|   vhall@example.org|11707 Donna Club ...|    East Danielshire|2012-03-11 00:00:00|     5433|\n",
      "|1007|       Jordan Morris|jennifermcpherson...|5740 Jennifer Way...| East Stephanieshire|1993-08-02 00:00:00|     1495|\n",
      "|1008|         Laura Moran| casey47@example.net|0414 William Over...|  Port Samanthamouth|2015-08-06 00:00:00|     3124|\n",
      "|1009|    Stacie Rodriguez|brandonjohnson@ex...|PSC 4090, Box 052...|        Kennethhaven|2020-02-23 00:00:00|     1861|\n",
      "|1010| Christopher Wallace|barnettregina@exa...|Unit 7686 Box 270...|           Paulatown|2010-05-27 00:00:00|     3385|\n",
      "|1011|         Paul Brooks|rebeccacarter@exa...|3360 David Common...|    Pattersonborough|2013-02-08 00:00:00|     2437|\n",
      "|1012|      Mrs. Amy Baker|  amyray@example.org|03557 Joseph Ridg...|        Carlachester|2020-08-22 00:00:00|     4831|\n",
      "|1013|   Christina Compton| james39@example.org|5616 King Vista\n",
      "B...|           Lake Brad|2004-05-29 00:00:00|     2057|\n",
      "|1014|         Andrew Ware|riggsalisha@examp...|90602 Mary Center...|        Hendrickston|2009-07-18 00:00:00|     2842|\n",
      "+----+--------------------+--------------------+--------------------+--------------------+-------------------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class Person\n",
       "personSchema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,false), StructField(name,StringType,true), StructField(email,StringType,true), StructField(address,StringType,true), StructField(city,StringType,true), StructField(dateTime,TimestampType,true), StructField(randomInt,IntegerType,false))\n",
       "data: org.apache.spark.sql.DataFrame = [id: int, name: string ... 5 more fields]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Person(id: Int,\n",
    "                  name: String,\n",
    "                  email: String,\n",
    "                  address: String,\n",
    "                  city: String,\n",
    "                  dateTime: java.sql.Timestamp,\n",
    "                  randomInt: Int)\n",
    "val personSchema = Encoders.product[Person].schema\n",
    "\n",
    "val data = spark.read\n",
    "  .format(\"csv\")\n",
    "  .schema(personSchema)\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"multiLine\", true)\n",
    "  .load(rawDataFile)\n",
    "\n",
    "data.show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc0c74e-722f-45c7-a887-28d35ed1c38d",
   "metadata": {},
   "source": [
    "Le jeu contient 500 000 enregistrements :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d069cecc-c129-4b79-b8a9-b8cc6e1de255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res5: Long = 10000\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7b029-3dc2-429f-a2d0-4b5dfafbb6ec",
   "metadata": {},
   "source": [
    "## Les tables au format Parquet\n",
    "Commençons par aborder la création et la manipulation de tables avec un format classique.\n",
    "\n",
    "Nous utilisons la syntaxe sql afin de créer un metastore (parfois appelé catalogue de metadata) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "694c5b8d-c58f-45c2-acde-8e227d9f1334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|deltalake_tuto_margo|\n",
      "+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "db: String = deltalake_tuto_margo\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val db = \"deltalake_tuto_margo\"\n",
    "spark.sql(s\"DROP DATABASE IF EXISTS ${db} CASCADE\")\n",
    "spark.sql(s\"CREATE DATABASE ${db}\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7ec5860-8ab0-4f59-97ba-1e7582c1b121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(s\"USE $db\")\n",
    "spark.sql(\"SHOW TABLES\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda18dcc-2111-42b0-bc11-63d857b7ec68",
   "metadata": {},
   "source": [
    "Et nous créons la table à partir des données chargées avec Spark :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf2b1c48-bd20-4080-aaab-380a9cd0dffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-----------+\n",
      "|            database|    tableName|isTemporary|\n",
      "+--------------------+-------------+-----------+\n",
      "|deltalake_tuto_margo|parquet_table|      false|\n",
      "+--------------------+-------------+-----------+\n",
      "\n",
      "+----+--------------------+--------------------+\n",
      "|id  |name                |city                |\n",
      "+----+--------------------+--------------------+\n",
      "|1000|David Farrell       |Colemanburgh        |\n",
      "|1001|Daniel Scott        |Port Jody           |\n",
      "|1002|Heather Thompson    |South Stephanieshire|\n",
      "|1003|Dr. Scott Morgan DDS|Danielberg          |\n",
      "|1004|Jennifer Torres     |Lake Jessica        |\n",
      "+----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.write\n",
    ".option(\"path\", parquetDataFolder)\n",
    ".saveAsTable(\"parquet_table\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show\n",
    "spark.sql(\"SELECT id, name, city FROM parquet_table LIMIT 5\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f72873-5747-43d0-9e26-937f18e9673c",
   "metadata": {},
   "source": [
    "La table parquet_table est maintenant contenue dans un fichier parquet (en conditions réelles, il y aura autant de fichiers parquets que de workers mais travailler avec un seul worker simplifie cette présentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7f82ea1-0180-4dca-9f57-f82c23e2c507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/data/parquet/part-00000-a2464fb9-ffc4-4fb6-bea6-7d8b7f79f497-c000.snappy.parquet 0.724852 MB\n",
      "/home/jovyan/data/parquet/_SUCCESS 0.0 MB\n"
     ]
    }
   ],
   "source": [
    "showFilesInDir(directoryParquet)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d61e721a-75d4-4f22-86ad-09ce4c7837ec",
   "metadata": {},
   "source": [
    "Afin de créer une opération fictive nous ajoutons à notre table les dix premières lignes de celle-ci. Les dix premières lignes sont donc en double dans la table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2be65f7-f684-4e91-af02-18021e9b81c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res10: org.apache.spark.sql.DataFrame = []\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\" \n",
    "             INSERT INTO parquet_table\n",
    "              SELECT * FROM parquet_table LIMIT 10;\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465df54-6b8e-4c9c-9446-e0446f0a4aa4",
   "metadata": {},
   "source": [
    "Spark va simplement créer un second fichier parquet qui contient les dix lignes que nous venons d’ajouter à la table :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1e6dc91-f560-417b-8533-b62f4d8bf8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/data/parquet/part-00000-a2464fb9-ffc4-4fb6-bea6-7d8b7f79f497-c000.snappy.parquet 0.724852 MB\n",
      "/home/jovyan/data/parquet/_SUCCESS 0.0 MB\n",
      "/home/jovyan/data/parquet/part-00000-0b0bf9dd-7961-4aae-8a1a-5fdcb31d2199-c000.snappy.parquet 0.003041 MB\n"
     ]
    }
   ],
   "source": [
    "showFilesInDir(directoryParquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36078ecd-bb0f-4d57-a3c1-475d688e95bb",
   "metadata": {},
   "source": [
    "Ce mode de stockage ne permet pas de connaître l’historique de la table car seul son état à l’instant T est disponible. L’exemple ci-dessus est simpliste mais imaginons un dossier de plusieurs centaines de fichiers sur une table régulièrement modifiée, il sera impossible de reconstituer un historique des transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae1271b9-693c-4dd6-9e62-528edc21f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   10010|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM parquet_table\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4003e81a-4f72-489f-b797-aa2811f47790",
   "metadata": {},
   "source": [
    "Par ailleurs, la commande suivante ne marchera pas en raison de l’immutabilité des fichiers parquet :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5ab69d8-acea-49d9-82ef-d34cae619d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "// spark.sql(\"DELETE FROM parquet_table WHERE id >= 480000\")\n",
    "// org.apache.spark.sql.AnalysisException: DELETE is only supported with v2 tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea632c40-35b5-4a43-b6b3-bf6d46031110",
   "metadata": {},
   "source": [
    "# Les tables au format Delta\n",
    "\n",
    "D’un point de vue purement pratique, Delta Lake s’utilise comme un format de fichier.\n",
    "\n",
    "Pour écrire une table, il suffit donc de remplacer .format(« parquet ») par .format(« delta ») :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e8875f",
   "metadata": {},
   "source": [
    "Error with DeltaLake, could be caused by a bug in spark?\n",
    "\n",
    "https://github.com/vericast/spylon-kernel/issues/40#issuecomment-318903262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e7bbdf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType}\n",
       "id: org.apache.spark.sql.types.StructField = StructField(id,IntegerType,true)\n",
       "name: org.apache.spark.sql.types.StructField = StructField(name,StringType,true)\n",
       "email: org.apache.spark.sql.types.StructField = StructField(email,StringType,true)\n",
       "address: org.apache.spark.sql.types.StructField = StructField(address,StringType,true)\n",
       "city: org.apache.spark.sql.types.StructField = StructField(city,StringType,true)\n",
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(id,IntegerType,true), StructField(name,StringType,true), StructField(email,StringType,true))\n",
       "df: org.apache.spark.sql.DataFrame = [id: int, name: string ... 1 more field]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType, TimestampType}\n",
    "\n",
    "val id = StructField(\"id\", IntegerType)\n",
    "val name = StructField(\"name\", StringType)\n",
    "val email = StructField(\"email\", StringType)\n",
    "val address = StructField(\"address\", StringType)\n",
    "val city = StructField(\"city\", StringType)\n",
    "//val dateTime = StructField(\"dateTime\", TimestampType)\n",
    "//val randomInt = StructField(\"randomInt\", IntegerType)\n",
    "\n",
    "\n",
    "val schema = StructType(Array(id, name, email))\n",
    "\n",
    "val df = spark.read.schema(schema).csv(rawDataFile)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adce4852-6cf8-43c0-a000-0045a5318987",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted.",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)",
      "  at org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:162)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:134)",
      "  at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:116)",
      "  at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:80)",
      "  at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:107)",
      "  at org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:106)",
      "  at org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:80)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.write(WriteIntoDelta.scala:106)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1(WriteIntoDelta.scala:65)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.$anonfun$run$1$adapted(WriteIntoDelta.scala:64)",
      "  at org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:188)",
      "  at org.apache.spark.sql.delta.commands.WriteIntoDelta.run(WriteIntoDelta.scala:64)",
      "  at org.apache.spark.sql.delta.sources.DeltaDataSource.createRelation(DeltaDataSource.scala:152)",
      "  at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:46)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)",
      "  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:90)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:127)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:126)",
      "  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:962)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:962)",
      "  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:414)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:345)",
      "  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:287)",
      "  ... 39 elided",
      "Caused by: java.lang.ArrayStoreException",
      ""
     ]
    }
   ],
   "source": [
    "df.write\n",
    ".format(\"delta\")\n",
    ".mode(\"overwrite\")\n",
    ".save(deltaDataFolder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e62b1a-1af4-4cea-b204-3d835a9712cd",
   "metadata": {},
   "source": [
    "Comme pour une table classique, Delta Lake permet l’utilisation de ces fonctionnalités via la syntaxe spark.sql :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f553ad-eb1e-491f-91dd-a1134427d0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(s\"\"\"\n",
    "    CREATE TABLE deltalake_table\n",
    "    USING DELTA\n",
    "    LOCATION '${deltaDataFolder}'\n",
    "  \"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM deltalake_table LIMIT 10;\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43c276-83fb-4946-99d2-aabc093847b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
